[ { "title": "Nordic Integration Summit", "url": "/posts/NordicIntegrationSummit/", "categories": "Azure, Public Speaking", "tags": "powershell, azure, devops, speaking, public, nis, nis24, nordic, integration, summit, nordic integration summit, västerås, user, group", "date": "2024-10-24 08:00:00 +0200", "snippet": "Nordic Integration SummitI attended the Nordic Integration Summit #NIS24 along with my colleagues to learn more about integration patterns that other companies implement and what we can expect of t...", "content": "Nordic Integration SummitI attended the Nordic Integration Summit #NIS24 along with my colleagues to learn more about integration patterns that other companies implement and what we can expect of the future in the field of integrations.The conference was arranged for the very first time and it really had some fantastic speakers.It ranged everywhere from sessions regarding: Infrastructure-as-code (Bicep session by bjompen) Monitoring using Open Telemetry - Using Open Telemtry to monitor at different layers of your architecture. Azure Architecture: Choosing wisely - Different architectures and designs, how to optimize performance and what does different choices result in. Building Sustainable Integration Solutions with Azure Integration Services - How to optimize your services while reducing your carbon footprint.just to mention a few! If you ever have the chance, you should really check these out.On the second day, we were really looking forward to a session being held by Stefan Ivemo deep-diving into private networking using private endpoints - what to think about, how to optimize design and what to expect from a large organisation - networkwise.However, Stefan could not attend and his time slot was left open. I reached out to the organisers of NIS24 and asked if I could present what me and my team has been working on - Subscription vending.It was approved about 2 and a half hours before the session was to be held, and I rushed to write a PowerPoint presentation along with preparing a demo.Luckily I was able to put everything together and I was ready to enter the stage to have my first public session. Västerås Azure User GroupIt was so much fun, I also had to present the same session at the Västerås Azure user group!" }, { "title": "Subscription Vending", "url": "/posts/SubscriptionVending/", "categories": "Azure, Subscription vending", "tags": "powershell, sebastian, claesson, azure, subscription, landing zone, vending, machine, vendingmachine, automation, permissions, ea, mca, rest", "date": "2024-10-06 02:55:00 +0200", "snippet": "IntroductionAn important part of Azure Landing Zones is the ability to create Azure subscriptions (landing zones) through automation. Any scripts and code used in this post can be found at: https:...", "content": "IntroductionAn important part of Azure Landing Zones is the ability to create Azure subscriptions (landing zones) through automation. Any scripts and code used in this post can be found at: https://github.com/SebastianClaesson/SubscriptionVendingExampleEnterprise Agreement Role Assignment for workload identitiesThere’s an article on Microsoft Learn that goes through how to assign a workload identity permissions to an Enterprise Agreement.If you are interested in following the least-privileged access model, then we must follow the article above to grant our workload identity access as “SubscriptionCreator” over our enrollment account.The process of subscription vending must not be bound to a employees account or permissions.However, not everyone is comfortable following the guide and take short-cuts such as assigning Enterprise Administrator over the billing account using the IAM controls in Azure.To assist with the creation of the EA Role assignment, I’ve created the following script New-EnterpriseAgreementRoleAssignmentOnce the assignment has been done, we need to build our subscription vending automation somewhere.This could be an Azure Function, GitHub/Azure DevOps Pipeline, Custom container or part of your self-service portal.Create your first Azure Subscription using PowerShellThe Az.Subscription PowerShell module contains the function “New-AzSubscriptionAlias” to provision a new Azure Subscription.We can simply write a function to provision the subscription using our workload identity.The function will run in the current logged in users context.[CmdletBinding()]param ( # BillingScope [Parameter(Mandatory)] [string] $BillingScope, # Workload [Parameter(Mandatory)] [string] $Workload, # ManagementGroupId [Parameter(Mandatory)] [string] $ManagementGroupId, # Identifier [Parameter(Mandatory)] [string] $Identifier, # Environment [Parameter(Mandatory)] [string] $EnvironmentShortName, # DisplayName [Parameter(Mandatory)] [string] $DisplayName)# The script requires the Az PowerShell Moduleif (! (Get-Module 'Az.Subscription' -ListAvailable)) { Throw 'Please install the Az PowerShell Module \"https://www.powershellgallery.com/packages/Az.Subscription\"'}Import-Module .\\Az.Subscription$params = @{ AliasName = \"$Identifier-$EnvironmentShortName\".toLower() SubscriptionName = \"$DisplayName-$EnvironmentShortName\".toLower() BillingScope = $BillingScope Workload = $Workload ManagementGroupId = $ManagementGroupId}Write-Verbose \"Attempting to list any Azure Subscription\" -Verbose$SubAliases = Get-AzSubscriptionAliasWrite-verbose \"Found a total of $($SubAliases.Count) Subscription Aliases.\" -Verbose$SubAliases | Select-Object AliasName, SubscriptionIdif ($SubAliases.AliasName -Contains \"$($params.AliasName)\") { $SubscriptionInfo = $SubAliases | Where-Object {$_.AliasName -eq \"$($params.AliasName)\"} Write-Verbose \"The subscription \"\"$($SubscriptionInfo.AliasName)\"\" already exists with id: '$($SubscriptionInfo.SubscriptionId)', Skipping creation.\" -Verbose} else { try { $SubscriptionInfo = New-AzSubscriptionAlias @params Write-Output $SubscriptionInfo Write-verbose \"Successfully created the subscription '$($SubscriptionInfo.AliasName)' with id: '$($SubscriptionInfo.SubscriptionId)'\" -Verbose } catch { throw }}Azure DevOps pipeline exampleConfigure Service Connection with Federated CredentialsTo utilize Azure DevOps for our Workload identity, we can utilize different types of authentication.I strongly recommend that federated credentials is configured to authenticate our workload identity.This means we do not have to manage a client secret, instead we trust the Azure DevOps directory to manage the credentials to our workload identity.As part of the service connection in Azure DevOps, we need to set a Azure Subscription where the pipeline initializes when running Azure PowerShell scripts.This simply can be done by providing for example the Reader role over a subscription, as we do not need access to any resources. You must register the resource providers for Management Groups and Subscriptions to successfully run the automation in your initial subscription.To save time, We’ll also create a script to manage the access and configuration of the service connection.## Requires the module ADOPSfunction New-ADOServiceConnection { [CmdletBinding()] param ( # SubscriptionId [Parameter(Mandatory)] [string] $SubscriptionId, # SubscriptionName [Parameter(Mandatory)] [string] $SubscriptionName, # Environment [Parameter(Mandatory)] [string] $Environment, # Identifier - This can be a project name, Service name or such to identify the Azure Landing Zone [Parameter(Mandatory)] [string] $Identifier, # Azure DevOps Organization [Parameter(Mandatory)] [string] $ADOOrganization, # Azure DevOps Project Name [Parameter(Mandatory)] [string] $ADOProjectName, # Role Defintion Name in Azure to be assigned to our Service Connection over the entire Landing Zone [Parameter(Mandatory)] [string] $RoleDefinitionName )}## Naming convention$AppRegistrationName = \"sc-azdo-$ProjectName-$Identifier-$Environment\"$ServiceConnectionName = \"sc-$Identifier-$Environment\"# The script requires the Az PowerShell Moduleif (! (Get-Module 'Az' -ListAvailable)) { Throw 'Please install the Az PowerShell Module \"https://www.powershellgallery.com/packages/Az\"'}# The script requires the ADOPS PowerShell Moduleif (! (Get-Module 'ADOPS' -ListAvailable)) { Throw 'Please install the ADOPS PowerShell Module \"https://www.powershellgallery.com/packages/ADOPS\"'}# Check if the user is already logged in to the Az PowerShell Module.$AzureContext = Get-AzContextif (!$AzureContext) { # User is not logged into Az PowerShell Module Write-verbose \"Please login to the Az PowerShell Module, this is used for confirming the existance of the Application Id and obtain an Azure Access Token\" -Verbose Connect-AzAccount}$Description = \"Federated Identity connection for Azure Subscription '$($AzureLandingZone.SubscriptionId)' as '$RoleDefinitionName'\"## Azure$TenantId = $AzureContext.Tenant.Idif ((Get-Module Az.Accounts).Version -lt '4.0.0') { $AccessToken = $(Get-AzAccessToken).Token}else { $AccessToken = $(Get-AzAccessToken).Token | ConvertFrom-SecureString -AsPlainText}# Verify that the Azure Landing zone exists.$AzureLandingZone = Get-AzSubscription -SubscriptionId $SubscriptionId# Connects to Azure DevOpsConnect-ADOPS -Organization $Organization -OAuthToken $AccessToken# Gets the Azure DevOps project$AzdoProject = Get-ADOPSProject -Name $ProjectNameif (!($AzdoProject)) { Get-ADOPSProject | Select-Object name, id | Sort-Object Name Throw \"Unable to find $ProjectName\"}# Gets the Azure DevOps Service Conection, if it already exists.$AdopsSC = Get-ADOPSServiceConnection -Name $ServiceConnectionName -Project $ProjectName -IncludeFailed -ErrorAction SilentlyContinueif (!($AdopsSC)) { $Params = @{ TenantId = $TenantId SubscriptionName = $SubscriptionName SubscriptionId = $SubscriptionId WorkloadIdentityFederation = $true Project = $ProjectName ConnectionName = $ServiceConnectionName.ToLower() CreationMode = 'Manual' Description = $Description } $AdopsSC = New-ADOPSServiceConnection @Params} else { Write-Verbose \"Found '$ServiceConnectionName' in the Project $ProjectName\" -Verbose}# Creates the Workload identity (Application Registration) using Az Module$EntraIdAppParams = @{ DisplayName = \"$AppRegistrationName\".tolower() Description = \"Azure DevOps Service Connection used in '$ProjectName' for credential federation.\" Confirm = $false}$App = Get-AzADServicePrincipal -DisplayName $EntraIdAppParams.DisplayNameif (!($App)) { $App = New-AzADServicePrincipal -AccountEnabled @EntraIdAppParams} else { Write-Verbose \"The Entra ID Service Principal '$($App.DisplayName)' already exists.\" -Verbose}$AppDetails = Get-AzADApplication -ApplicationId $App.AppId# Creates Entra Id Federated Credentials for authentication between Azure DevOps and Entra id using our Workload identity$FederatedCreds = Get-AzADAppFederatedCredential -ApplicationObjectId $AppDetails.Idif ($AdopsSC.authorization.parameters.workloadIdentityFederationSubject -in $FederatedCreds.Subject) { Write-Verbose \"Azure DevOps Federated Credentials have already been configured.\" -Verbose} else { $FederatedCredentialsParams = @{ ApplicationObjectId = $AppDetails.Id Issuer = $AdopsSC.authorization.parameters.workloadIdentityFederationIssuer Subject = $AdopsSC.authorization.parameters.workloadIdentityFederationSubject Name = 'AzureDevOpsAuthentication' Description = \"Azure DevOps Federated Credentials\" Audience = 'api://AzureADTokenExchange' } New-AzADAppFederatedCredential @FederatedCredentialsParams}# Removes the default client secret$Secret = Get-AzADAppCredential -ObjectId $AppDetails.Idif ($Secret) { Remove-AzADAppCredential -KeyId $Secret.KeyId -ApplicationId $AppDetails.AppId}# Assigning correct permissions to Azure Landing Zone.if (!(Get-AzRoleAssignment -Scope \"/subscriptions/$($AzureLandingZone.SubscriptionId)\" -RoleDefinitionName $RoleDefinitionName -ObjectId $App.Id)) { New-AzRoleAssignment -Scope \"/subscriptions/$($AzureLandingZone.SubscriptionId)\" -RoleDefinitionName $RoleDefinitionName -ObjectId $App.Id} else { Write-Verbose \"'$($App.Id)' already has access as '$RoleDefinitionName' over subscription '$($AzureLandingZone.SubscriptionId)'\" -Verbose}# Completes the Service connection authentication details in Azure DevOps$Params = @{ TenantId = $TenantId SubscriptionName = $subscriptionName SubscriptionId = $subscriptionId Project = $ProjectName ServiceEndpointId = $AdopsSC.Id ConnectionName = $AdopsSC.name ServicePrincipalId = $App.AppId WorkloadIdentityFederationIssuer = $AdopsSC.authorization.parameters.workloadIdentityFederationIssuer WorkloadIdentityFederationSubject = $AdopsSC.authorization.parameters.workloadIdentityFederationSubject Description = $Description}Set-ADOPSServiceConnection @Params Do not forget to limit access to the service principal. If you intend to let non-platform engineers provision and create landing zones then perhaps a good option is to configure approval for the use of the service connection.Azure DevOps .yml exampleOnce this is done, we can write our Azure DevOps pipeline.We will start with just creating our new Landing Zone and setting our prefered Management group.trigger: none variables: - name: subscriptionCreationServiceConnection value: 'SERVICECONNECTIONNAME'parameters:- name: Identifier displayName: What is the identifying name of the Azure Landing Zone? (short name) type: string- name: DisplayName displayName: What is the display name of the Azure Landing Zone? (long name) type: string- name: ManagementGroupName displayName: Azure Management Group Name type: string- name: BillingScope displayName: Azure Billing Scope, Example /billingAccounts/123456/enrollmentAccounts/123456 type: string- name: Workload displayName: Azure Subscription Workload offer - https://azure.microsoft.com/en-us/pricing/offers/dev-test type: string values: - Production - DevTest- name: Environment displayName: Environment? type: string default: Sandbox values: - sbx - dev - acc - prodstages: - stage: provision displayName: 'Subscription Vending' jobs: - job: subscriptionCreate displayName: 'Create Azure Subscription' steps: - task: AzurePowerShell@5 displayName: 'Create Azure Subscription' name: CreateSub inputs: azureSubscription: $(subscriptionCreationServiceConnection) ScriptType: 'FilePath' azurePowerShellVersion: LatestVersion pwsh: true ScriptPath: 'New-AzureSubscription.ps1' ScriptArguments: &gt; -Identifier '$`\\{`\\{ parameters.Identifier `\\}}' -BillingScope '$`\\' -Workload 'Production' -ManagementGroupId '/providers/Microsoft.Management/managementGroups/${`\\{ parameters.ManagementGroupName }}' -EnvironmentShortName '$(variableOutput.environmentShortName)' -DisplayName '$'After importing and running the Azure DevOps pipeline, the output should simply look like this:Now we can add more steps to the pipeline according to our needs.ConclusionWe have now established a workload identity with federated credentials to provision our Azure Subscriptions.We have also created the nessecary automation &amp; Azure DevOps pipeline to provide a basic self-service feature for our colleagues.We can continue to add steps to our subscription vending, for example incorporating the orchestration of Entra Id security groups, privileged access management, entitlement management, IP address management, critical infrastructure resources such as peering to hub network, budgets, service health alerts and so on.References; Subscription vending implementation guidance Subscription vending Azure devops workload identity federation" }, { "title": "Azure AD Cross-tenant Collaboration / VNET Cross-tenant peering", "url": "/posts/AzureCrossTenantCollaboration/", "categories": "Azure, Cross-tenant collaboration", "tags": "powershell, azure, azure ad, network, cross-tenant, b2b", "date": "2023-04-03 14:55:00 +0200", "snippet": "The possibility to create Virtual network peerings across Azure Active Directory tenants has been available since 2018. It’s a feature which is allowed by default and is quite easy to setup and get...", "content": "The possibility to create Virtual network peerings across Azure Active Directory tenants has been available since 2018. It’s a feature which is allowed by default and is quite easy to setup and get started with. It helps provide private networking between two Azure AD tenants subscriptions and can be an alternative to private link/private endpoints etc. This post will go through the setup and requirements, but also how you can detect cross-tenant collaboration &amp; blocking it. I will not deep-dive into all the toolings around it, such as Conditional access or other features you get available by using Azure AD Premium.Azure AD settings &amp; invitation processTenant A is a new Tenant, out-of-box, with no customization done to External collaboration settings.Out of box, external collaboration settings: This means that anyone in the Tenant A organization can invite guest users and that it may be sent to any domain. If we head over to Cross-tenant access settings, we can also see that B2B collaboration access settings are allowed both inbound and outbound. Reference: Cross-tenant access settingsWe attempt to reach the Azure AD Tenant A from the user in Tenant B, without being invited as a guest to that tenant.If you are inviting a user that does not have an E-mail address, you can use for example PowerShell to create and consume an invitation. Reference: Invite guest user - PowerShell$params = @{ InvitedUserDisplayName = \"Sebastian Guest user\" InvitedUserEmailAddress = \"sebastian.claesson@contoso.com\" InviteRedirectUrl = \"https://myapplications.microsoft.com\" SendInvitationMessage = $false}$Invite = New-MgInvitation @params$invite | select-object Id, InviteRedeemUrl, InvitedUserDisplayName, InvitedUserEmailAddress, InvitedUserType, Status, @{'n'='UserId';'E'={$_.InvitedUser.Id}} | ConvertTo-JsonThe response from the invitation is as following:{ \"Id\": \"51b5db20-6630-4776-a183-288f78dff904\", \"InviteRedeemUrl\": \"https://login.microsoftonline.com/redeem?rd=https%3a%2f%2finvitations.microsoft.com%2fredeem%2f%3ftenant%3d31ca78c2-d833-433a-9977-88e160bd4ac0%26user%3d51b5db20-6630-4776-a183-288f78dff904%26ticket%3dHq53w18dK78ZhChJlhx37EDD9i3xIQBIUc9uDoqurwQ%25253d%26ver%3d2.0\", \"InvitedUserDisplayName\": \"Sebastian Guest user\", \"InvitedUserEmailAddress\": \"sebastian.claesson@contoso.com\", \"InvitedUserType\": \"Guest\", \"Status\": \"PendingAcceptance\", \"UserId\": \"a9cf0759-4de1-4ee2-85a6-47ff8f4e34f0\"}Once we consume the InviteRedeemUrl, we simply go to the invited user’s context and browse to the link.Once the invitation has been accepted, we can verify that the user has been created in Tenant A by runningGet-MgUser -userId $Invite.InvitedUser.Id# OutputId : a9cf0759-4de1-4ee2-85a6-47ff8f4e34f0DisplayName : Sebastian ClaessonUserPrincipalName : sebastian.claesson_contoso.com#EXT#@tenantA.onmicrosoft.comLets try to establish a network peering from Tenant B to Tenant A using the Portal experience.We receive the error:Failed to add virtual network peering 'TenantB2TenantA' to 'vnet-demo-test'. Error: The client 'sebastian.claesson@contoso.com' with object id '30d70e94-10e8-494d-bece-ea990d773b47' has permission to perform action 'Microsoft.Network/virtualNetworks/virtualNetworkPeerings/write' on scope 'demo-test-rg/providers/Microsoft.Network/virtualNetworks/vnet-demo-test/virtualNetworkPeerings/TenantB2TenantA' &gt; vnet-demo-test/TenantB2TenantA'; however, it does not have permission to perform action 'peer/action' on the linked scope(s) '/subscriptions/8655f4d3-0bb1-4be0-b73c-6a8f3b304cf6/resourceGroups/demo-rg/providers/Microsoft.Network/virtualNetworks/demo-vnet' or the linked scope(s) are invalid.This means we’re only invited as a guest with no Azure Resource access in target subscription. We’ll assign our Tenant B user the role of “Network Contributor” in the correct subscription of Tenant A and make a new attempt. To confirm that the peering has been initiated we can run the following command:Get-AzVirtualNetworkPeering -VirtualNetworkName \"vnet-demo-test\" -ResourceGroupName \"demo-test-rg\" | Select Name, @{'N'='RemoteVnetId';'E'={$_.RemoteVirtualNetwork.Id}}, PeeringSyncLevel, PeeringState, ProvisioningState | flName : TenantB2TenantARemoteVnetId : /subscriptions/8655f4d3-0bb1-4be0-b73c-6a8f3b304cf6/resourceGroups/demo-rg/providers/Microsoft.Network/virtualNetworks/demo-vnetPeeringSyncLevel : RemoteNotInSyncPeeringState : InitiatedProvisioningState : SucceededAs indicated, the peering has been initiated, however not completed.When checking the status of the remote virtual network we are trying to peer with, we can see that there’s been no peering created. This means we’ll have to estalish a peer from Tenant A to Tenant B.The easiest way is simply to change tenant in the invited user’s context and establish the peering using the portal experience.To confirm a successful peering, we can run the following PowerShell commandGet-AzVirtualNetworkPeering -VirtualNetworkName 'demo-vnet' -ResourceGroupName 'demo-rg' | Select Name, @{'N'='RemoteVnetId';'E'={$_.RemoteVirtualNetwork.Id}}, @{'N'='RemoteVirtualNetworkAddressSpace';'E'={$_.RemoteVirtualNetworkAddressSpace.AddressPrefixes}}, PeeringSyncLevel, PeeringState, ProvisioningState | fl# OutputName : TenantA2TenantBRemoteVnetId : /subscriptions/31bdf544-8e8e-4f5b-bb31-3316a05e5581/resourceGroups/demo-test-rg/providers/Microsoft.Network/virtualNetworks/vnet-demo-testRemoteVirtualNetworkAddressSpace : 192.168.1.0/24PeeringSyncLevel : FullyInSyncPeeringState : ConnectedProvisioningState : SucceededGet-AzVirtualNetworkPeering -VirtualNetworkName \"vnet-demo-test\" -ResourceGroupName \"demo-test-rg\" | Select Name, @{'N'='RemoteVnetId';'E'={$_.RemoteVirtualNetwork.Id}}, @{'N'='RemoteVirtualNetworkAddressSpace';'E'={$_.RemoteVirtualNetworkAddressSpace.AddressPrefixes}}, PeeringSyncLevel, PeeringState, ProvisioningState | flName : TenantB2TenantARemoteVnetId : /subscriptions/8655f4d3-0bb1-4be0-b73c-6a8f3b304cf6/resourceGroups/demo-rg/providers/Microsoft.Network/virtualNetworks/demo-vnetRemoteVirtualNetworkAddressSpace : 10.0.0.0/16PeeringSyncLevel : FullyInSyncPeeringState : ConnectedProvisioningState : SucceededHere we can confirm that the peering between the tenants have been established successfully.These networks can now access each other and routes has been presented by the peer, as seen in the following screenshot on a NIC inside Tenant B.How do we protect our Organization for cross-tenant collaboration?You might ask yourself “How do we protect our employees/organization from accidently allowing cross-tenant virtual network peerings to prevent data exfiltration or network communication by-passing the VNA?”.There are different ways to enforce and detect this some examples could be a controlled process for when BGP is enabled on route tables. However, if we assume that both Tenants/Organizations have adopted the Landing Zone concept, we know that our developers/users have access to modify route tables, network peerings etc. to manage their LZ. In this part we’ll focus on how we can prevent cross-tenant collaboration and not on suggested Azure policies to enforce controlled processes around infrastructure/network changes. We’ll have to go back to the Cross-tenant access settings, specfically the Inbound and Outbound B2B collaboration settings. Note: B2B collaboration settings are not only limited to Azure resource actions, and can include Office 365 and other services.Reference: Azure AD B2B CollaborationOutbound blockedIf we set the Outbound access settings for B2B collaboration to “All blocked”, we can still establish a network peering by following the procedure above.Inbound blockedIf we set the Inbound access settings for B2B collaboration to “All blocked”, we are unable to establish a peer from Tenant B to Tenant A.We are also unable to login towards the tenant without guest user account. Receiving the error message above, also with the detailed error “AADSTS500213: The resource tenant’s cross-tenant access policy does not allow this user to access this tenant.”DetectionThere’s a workbook published in Azure AD that you can utilize if logs are streamed to a log analytics workspace.It’s called “Cross-tenant access activity”To get a bit more in-depth data on the activity, you can simply take a snippet out of the query such as:SigninLogs | project TimeGenerated, UserPrincipalName, HomeTenantId, AADTenantId, Id, ResourceTenantId, ResourceDisplayName, ResourceIdentity, Status, UserType, UserId | where UserId != \"00000000-0000-0000-0000-000000000000\" | where ResourceIdentity != '' | where \"All users\" == \"All users\" or UserPrincipalName has \"All users\" | where \"All applications\" == \"All applications\" or ResourceDisplayName has \"All applications\" | where \"All external tenants\" == \"All external tenants\" or HomeTenantId has \"All external tenants\" | where HomeTenantId != '' | where HomeTenantId != AADTenantId | extend status = case(Status.errorCode == 0, \"Success\", \"Failure\") | where \"All\" == status or \"All\" == \"All\"Now you’ll visualize each request and you are able to identify each tenant that your users are interacting with.SummaryIt’s a good idea to keep track of possible cross-tenant integrations, may it be using virtual network peerings / private links / private endpoints or by using service endpoints. This post only covers some of these points and how it can be prevented using other methods than Azure policy or Azure monitor/alert. This post resulted in a pull request to update Microsoft Learn documentation on Pull Request 107640." }, { "title": "Azure DevOps Pipeline Agent Extension (Ubuntu) - Behind a web proxy.", "url": "/posts/AzureDevOpsExtension-Linux/", "categories": "Azure DevOps, DevOps Agent", "tags": "powershell, azure, devops, extension, virtual machine, vmss, proxy, ubuntu", "date": "2022-11-08 13:55:00 +0100", "snippet": "Are you looking to run your Azure DevOps agent behind an unauthenticated/authenticated web proxy for traffic destined to the internet?Then hopefully this post will help you straighten out those que...", "content": "Are you looking to run your Azure DevOps agent behind an unauthenticated/authenticated web proxy for traffic destined to the internet?Then hopefully this post will help you straighten out those question marks that might come up during the process.There’s a bit of a difference between the Windows and Linux agent, this post will only cover the Linux agent running on Ubuntu 20.04, one for the Windows Agent is on it’s way.We’ll start first with a summary of how the installation and deployment of the Azure DevOps VM Extension works, this will help us get an insight of what steps in the process where issues might occur. VM/VMSS has the Azure DevOps Extension deployed to it, using the portal, cli or infrastructure-as-code. The VM/VMSS will download the extension in a compressed format (zip) from a public Azure Storage Account. The Extension will run the “Handler.sh -enable” command and run either AzureRM.py or AzureRM_Python2.py (depending on what Python version is available) to install the DevOps Agent. AzureRM.py (or AzureRM_Python2.py) will read the settings file (containing the Public and Protected Settings), decrypt the protected settings with the computer certificate available and remove it from the settings file. AzureRM.py will download the Azure DevOps agent zip file and EnableAgent script specified in the Public settings. The InstallDependecies.sh script will use APT to install missing dependencies. The Azure DevOps agent installation will start and configure itself according to the scenario specified.The log locations for the Azure DevOps VM Extension are: /var/log/azure/Microsoft.VisualStudio.Services.TeamServicesAgentLinux /agent directory/_diag (The value for the directory per default is “agent”) Read more about the communication and setup of Azure DevOps AgentAs part of our network design, it is decided that no web traffic may go directly to the internet destination, instead it will always go through a proxy.If the traffic tries to reach it’s destination without going through the proxy, the network security group (NSG) will stop it.However the NSG will not interrupt web traffic within the virtual network. I highely suggest that you use the Squid Proxy Server in Azure Marketplace to experiment with, Note that the product has an hourly cost.The first thing we will have to do is to create the Bicep template. VMSS Bicep Template Example var location = resourceGroup().locationvar AzureDevOpsPATToken = 'AzureDevOpsPATToken'var subnetId = '/subscriptions/81bee834-3e8e-4f5d-bb31-3316a05e5583/resourceGroups/demo-rg/providers/Microsoft.Network/virtualNetworks/demo-vnet/subnets/demo-snet'resource vmss 'Microsoft.Compute/virtualMachineScaleSets@2022-03-01' = { name: 'demo-vmss' location: location identity: { type: 'SystemAssigned' } sku: { capacity: 1 name: 'Standard_B2ms' } properties: { overprovision: false upgradePolicy: { mode: 'Manual' } singlePlacementGroup: false virtualMachineProfile: { storageProfile: { osDisk: { createOption: 'FromImage' } imageReference: { publisher: 'canonical' offer: '0001-com-ubuntu-server-focal' sku: '20_04-lts-gen2' version: 'latest' } } osProfile: { adminPassword: 'DemoTest123' adminUsername: 'demouser' computerNamePrefix: 'demo' } networkProfile: { networkInterfaceConfigurations: [ { name: 'demo-vmss-nic' properties: { primary: true enableAcceleratedNetworking: false ipConfigurations: [ { name: 'demo-vmss-nic-ipc' properties: { primary: true subnet: { id: subnetId } } } ] } } ] } extensionProfile: { extensions:[ { name: 'Microsoft.Azure.DevOps.Pipelines.Agent' properties: { autoUpgradeMinorVersion: false publisher: 'Microsoft.VisualStudio.Services' type: 'TeamServicesAgentLinux' typeHandlerVersion: '1.23' settings: { isPipelinesAgent: true agentFolder: '/agent' AzureDevOpsOrganizationUrl: 'https://dev.azure.com/sebcla' TeamProject: 'Demo' enableScriptParameters: 'https://dev.azure.com/sebcla AgentPoolName ${AzureDevOpsPATToken}' agentDownloadUrl: 'https://vstsagentpackage.azureedge.net/agent/2.211.1/vsts-agent-linux-x64-2.211.1.tar.gz' enableScriptDownloadUrl: 'https://vstsagenttools.blob.core.windows.net/tools/ElasticPools/Linux/14/enableagent.sh' } protectedSettings: { PATToken: AzureDevOpsPATToken } } } ] } } }} Currently rouge do not support Bicep syntax highlighting. If you would like to see Bicep hightlighting, then please up-vote Rouge Bicep Hightlighting at GitHub. Note that in the example ARM file a Personal Access Token (PAT) is used under the protected settings. You should never share your PAT with anyone and always keep it protected, therefor it is not a good practice to use the PAT in clear text of your deployment. Please make sure you only issue short-lived tokens if you are to use them in clear text or use a keyvault reference.Read more about that here: Microsoft.Compute/virtualMachines/extensionsThe first issue we will encounter is that the Azure DevOps Extension will fail it’s installation.This is because the Azure DevOps VM Extension cannot be downloaded as it is hosted on Microsoft generated storage accounts.In my case the url was: https://umsaqts1kdw3dgdrdmzt.blob.core.windows.net/76d90c30-c607-43bc-49aa-02e322a01e7b/76d92c30-c607-43bc-49aa-32e322a01e7b_1.22.0.0.zipIf you want to see more details about the VM Extension logs, you can find them at /var/log/azure/Microsoft.VisualStudio.Services.TeamServicesAgentLinux/CommandExecution.logExample log of a successful download:2022-11-04T08:55:06.150100Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] Target handler state: enabled [incarnation_1]2022-11-04T08:55:06.150369Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] [Enable] current handler state is: notinstalled2022-11-04T08:55:06.150627Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] Downloading extension package: https://umsaqts1kdw3dgdrdmzt.blob.core.windows.net/76d90c30-c607-43bc-49aa-02e322a01e7b/76d92c30-c607-43bc-49aa-32e322a01e7b_1.22.0.0.zip2022-11-04T08:55:06.187235Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] Unzipping extension package: /var/lib/waagent/Microsoft.VisualStudio.Services.TeamServicesAgentLinux__1.22.0.0.zip2022-11-04T08:55:06.191683Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] Initializing extension Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.02022-11-04T08:55:06.192513Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] Update settings file: 266.settings2022-11-04T08:55:06.192717Z INFO ExtHandler [Microsoft.VisualStudio.Services.TeamServicesAgentLinux-1.22.0.0] Install extension [Handler.sh] As you can see in the logs, this is where the settings file is generated.We will go deeper into the settings file, what it is used for and when further down.Once you have allowed the Azure DevOps VM extension to download itself, then the next problem will be that the Azure DevOps VM Extension itself is not proxy aware and fail when trying to download the agent zip and enable agent script.Looking at the code for the Azure DevOps Extension, we can see on row 596 in the AzureRM.py that it will try to run the command: “Util.url_retrieve(downloadUrl, agentFile)”.The downloadUrl and agentFile is defined in the VM Extension part of your scale-set.{ name: 'Microsoft.Azure.DevOps.Pipelines.Agent' properties: { autoUpgradeMinorVersion: false publisher: 'Microsoft.VisualStudio.Services' type: 'TeamServicesAgentLinux' typeHandlerVersion: '1.22' settings: { isPipelinesAgent: true agentFolder: '/agent' AzureDevOpsOrganizationUrl: 'https://dev.azure.com/sebcla' TeamProject: 'Demo' enableScriptParameters: 'https://dev.azure.com/sebcla AgentPoolName ${AzureDevOpsPATToken}' agentDownloadUrl: 'https://vstsagentpackage.azureedge.net/agent/2.211.1/vsts-agent-linux-x64-2.211.1.tar.gz' enableScriptDownloadUrl: 'https://vstsagenttools.blob.core.windows.net/tools/ElasticPools/Linux/14/enableagent.sh' } protectedSettings: { PATToken: AzureDevOpsPATToken } }}The Util library is imported from the Utils/HandlerUtil.py module.The url_retrieve function contains the following bit of code:def url_retrieve(download_url, target): if ('ProxyUrl' in proxy_config): proxy_url = proxy_config['ProxyUrl'] proxy_handler = urllib.request.ProxyHandler({'https': proxy_url}) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener urllib.request.urlretrieve(download_url, target))Which suggests that the function should be retreiving the proxy settings, if ProxyUrl is defined in “proxy_config”.Going back to the AzureRM.py script, we can see that the proxy_config is imported as ‘from Utils.GlobalSettings import proxy_config’.Reading the GlobalSettings.py file we can see that it contains the following:proxy_config = {}However, there is no function where this file gets populated on the fly depending on for example the machine variables http_proxy/https_proxy etc. Looking at the urllib documentation urllib docs we can see that it supports a function of getting the proxy settings “request.getproxies()”, but as part of the code it is not implemented.As we cannot make the Extension understand that it should use a proxy, we can edit the path for agentDownloadUrl and enableScriptDownloadUrl to be hosted on a Azure Storage Account with a private endpoint in the same virtual network as the VMSS.This will allow the Azure DevOps Extension to download these files without a web proxy.As we are allowed to send web traffic within our virtual network without having the traffic dropped.{ name: 'Microsoft.Azure.DevOps.Pipelines.Agent' properties: { autoUpgradeMinorVersion: false publisher: 'Microsoft.VisualStudio.Services' type: 'TeamServicesAgentLinux' typeHandlerVersion: '1.22' settings: { isPipelinesAgent: true agentFolder: '/agent' AzureDevOpsOrganizationUrl: 'https://dev.azure.com/sebcla' TeamProject: 'Demo' enableScriptParameters: 'https://dev.azure.com/sebcla AgentPoolName ${AzureDevOpsPATToken}' agentDownloadUrl: 'https://internaltestfeed.blob.core.windows.net/devops/vsts-agent-linux-x64-2.211.1.tar.gz' enableScriptDownloadUrl: 'https://internaltestfeed.blob.core.windows.net/devops/enableagent.sh' } protectedSettings: { PATToken: AzureDevOpsPATToken } }}Once you have successfully updated the extension settings, you can now read the log file @ /agent/_diag/Agent_timestamp-utc.log which contains the following rows that identifies that we have set the proxy correctly.Correct setup[2022-11-04 08:23:39Z INFO AgentProcess] Arguments parsed[2022-11-04 08:23:39Z INFO HostContext] Well known directory 'Bin': '/agent/bin'[2022-11-04 08:23:39Z INFO HostContext] Well known directory 'Root': '/agent'[2022-11-04 08:23:39Z INFO HostContext] Well known config file 'Proxy': '/agent/.proxy'[2022-11-04 08:23:39Z INFO VstsAgentWebProxy] Config proxy at: http://10.2.0.7:3128.[2022-11-04 08:23:39Z INFO HostContext] Well known directory 'Bin': '/agent/bin'[2022-11-04 08:23:39Z INFO HostContext] Well known directory 'Root': '/agent'[2022-11-04 08:23:39Z INFO HostContext] Well known config file 'ProxyCredentials': '/agent/.proxycredentials'[2022-11-04 08:23:39Z INFO VstsAgentWebProxy] Config proxy use DefaultNetworkCredentials.If the string is not a correct formated string (https/http://proxyaddress:port)[2022-11-04 07:48:33Z INFO AgentProcess] Arguments parsed[2022-11-04 07:48:33Z INFO HostContext] Well known directory 'Bin': '/agent/bin'[2022-11-04 07:48:33Z INFO HostContext] Well known directory 'Root': '/agent'[2022-11-04 07:48:33Z INFO HostContext] Well known config file 'Proxy': '/agent/.proxy'[2022-11-04 07:48:33Z ERR VstsAgentWebProxy] The proxy url is not a well formed absolute uri string: 10.2.0.7:3128.[2022-11-04 07:48:33Z INFO VstsAgentWebProxy] No proxy setting found.Now the Azure DevOps agent can report back to Azure DevOps using the web proxy!Maybe now you have started to wonder - I have issued an Azure DevOps token, where and how is it stored?Well, to be honest I have not yet figured exactly how it works, but I’ve noticed that the token is used to issue a JWT towards Azure DevOps.The JWT issued is only valid for a short time and can be used to report back as a healthy agent to Azure DevOps.The settings file (containing your JWT and other settings) will be inserted into the VM/instance you are running and available on disk.The path of the settings file:/var/lib/waagent/Microsoft.VisualStudio.Services.TeamServicesAgentLinux-versionnumber/config/uniquenumber.settingsThe settings files contains the ProtectedSettings and Settings attribute of the extension, meaning the the token/JWT is available inside of the file.If you want to decrypt it manually, it is possible by using the Python module “HandlerUtil.py” as it contains a function to decode the settings using the computer certificate.The code for it:_parse_config(self, ctxt, operation): config = None try: config=json.loads(ctxt) except: self.error('JSON exception decoding ' + ctxt) if config == None: self.error(\"JSON error processing settings file:\" + ctxt) else: handlerSettings = config['runtimeSettings'][0]['handlerSettings'] if 'protectedSettings' in handlerSettings and \\ \"protectedSettingsCertThumbprint\" in handlerSettings and \\ handlerSettings['protectedSettings'] is not None and \\ handlerSettings['protectedSettings'] != '' and \\ handlerSettings[\"protectedSettingsCertThumbprint\"] is not None: protectedSettings = handlerSettings['protectedSettings'] thumb=handlerSettings['protectedSettingsCertThumbprint'] cert=waagent.LibDir+'/'+thumb+'.crt' pkey=waagent.LibDir+'/'+thumb+'.prv' waagent.SetFileContents('/tmp/kk', protectedSettings) cleartxt=None cleartxt=waagent.RunGetOutput(\"base64 -d /tmp/kk | openssl smime -inform DER -decrypt -recip \" + cert + \" -inkey \" + pkey )[1] os.remove(\"/tmp/kk\") if cleartxt == None: self.error(\"OpenSSh decode error using thumbprint \" + thumb ) self.do_exit(1,operation,'error','1', operation + ' Failed') jctxt='' try: jctxt=json.loads(cleartxt) except: self.error('JSON exception decoding ' + cleartxt) handlerSettings['protectedSettings']=jctxt self.log('Config decoded correctly.') return configExample of settings file on disk.{ \"runtimeSettings\": [ { \"handlerSettings\": { \"protectedSettingsCertThumbprint\": \"C995E47CEFBD87EBA02B01E1BBFBA6D1A6E83352\", \"protectedSettings\": \"MIIF3AYJKoZIhvcNAQcDoIIFzTCCBckCAQAxggFpMIIBZQIBADBNMDkxNzA1BgoJkiaJk/IsZAEZFidXaW5kb3dzIEF6dXJlIENSUCBDZXJ0aWZpY2F0ZSBHZW5lcmF0b3ICEE5Dxku1ulysRjLkep1nhIcwDQYJKoZIhvcNAQEBBQAEggEAf1+0EYEt/c4yDLdQTQemZMyWF8pUrjJuM222sHXiqQvzpzi0L+ezgRFQw+/3cr0QXUytuEOLsrWZcEziOONhZhaCaSD+58JRwU+Z006nJxg+rM06EP3ff12J36PfYR5/+YDYgs66uG6A+S3Y2ZoMczbCpRItH0iCeyFOnPSRGpWISqFA9tpw+HpqHWlY5WM8ugngsQSRPeUFtjb62e9z+LWvJq6z0nh7oIMUMLB/jCEIfUi7eH3egMLHZHaX+QD+O0B7gXD+L5d5a4+AXkkZle0eVuzdUJGgcwPGf56xGMYikHx0Yfuc9QtV+bCzRAcahalKE1WflyEmZU6aZHOrPjCCBFUGCSqGSIb3DQEHATAUBggqhkiG9w0DBwQIbICCIhCAggQw0v+55ijfEDXiy7Tl+4JsG4pYVW1G2iI9iAXqAL4LcVFEvWE3+Q1UY59S34tbJAxxMbuoZX8ij97gE0+icQZL/sq6M3x0QXK+XGrIkG5nW/mGta7OGV/DKAaZt5jlKzgWq2klCrVSc5Xm3nuh+5mU3EltcAvc4q7wRsx4LWXC8AL4PIcD1SnjUrn7M87QrlMxG6GL3iSJEA8tUXDVgdoQKgUiatnQXOpTIcIiOIdFKrYJxbePUUSN5qyAf8rXZ4Ta9kp3xR0AdXtPQ0SHAbd+LaVztXzkbG8l7PnV4KJNXZCo4Vob7dN4crE7/abZZDV5FpWaWcZIeBIdcqAItHvFtQ8Rq9xwb/urI6vdbUZeVaI61+JLqp3ZLhIfzeejOtlH3HxQk4xofg+br5QEx2k+k47HAZE5xh5xRQwH6G5sSsYfDvFvFBmdaEJByfvh2Tv0a4JqYKoANjCVBvu5zAl88MukkIloxc1rdGtIyvBDC0uWYf2gFAE92g2a+B8EaYDFxOLQV8ZC1k1ZCtLQiYe/NdtbsUJnGPesdJ1sBLdmidclPO6gyB/HAXoJgdmx7ppFylO0A3RJnGyc85oORhc/qPkicT1a61aZlg+MrCwolQ2ImBZrzMCMWK/smgNEGJYa8ksPjhY3Mb1QHDFOc8uzQCFDhQI+F6psNbuvYrPdm4Jjcb5vIdlojEBMuWqbFwHX/29tPU/5tMZ2H+tABzI9b1CbdY5kbmTUQfX3+tQladdNF8hvS1WLgYuvOMpdn9sYiwOvmsKf8oolrzoQ4Occ2eAMhQDWAWyWr1M2E0KB6dEMQApwAoVVdPWicf+1WIMzyY6MoRW1FZbMtkqJDiZYJo0zmSmnpK1tCZWKh+1m83o5b5U34doFa7TZbRCjdDL1CYTEBQWGJdFHFh0ktqlg7pqihy6cpjJOhnEEaEEX74cTVx9nFCmAQPW/4dYeplq5gbT188/+kd5dKTcckp4uHLvH4+m2f09qp5Vv1vvYmenSn/B5uI9l4fr21kVuSaTyF3bEt9ajHKXho8sgHP2lRmla0pyRVPU/eEClCsXpJxKsYehQ76NA8jG51GvjUm5T1ZKPRxk94p98pWTjeAZ+6jJ0t0nG20tFST1YauQlj2Px9t1X3KQRwyO3rzlCKoMAVYtAykUsXesgIKt42FXqzbxkeVRIARkSHA2g2ELwYFsrZEZUa6zG3g5829vknWax6dDDe78CyjWE0EmdcZqlJMpEcEeUl/ObsgNnubPS8RLRO9ZSbvRAhTTUY7vhuH6O9zW9E+Awmg+nWHXKn6o+HGc/2S/jNdYaUPzZNwoHzYafoYQAuMonRs+PIpyNOSlw1ixEwyjqDRop0rGCHb0gj07yNp3XykXx0lIlEaNa1oPDQNey0NZo4pL6QJZ0/pA==\", \"publicSettings\": { \"isPipelinesAgent\": true, \"agentFolder\": \"/agent\", \"agentDownloadUrl\": \"https://internaltestfeed.blob.core.windows.net/devops/vsts-agent-linux-x64-2.211.1.tar.gz\", \"enableScriptDownloadUrl\": \"https://internaltestfeed.blob.core.windows.net/devops/enableagent.sh\" } } } ]}I have noticed in my tests that if you revoke or let the PAT Token expire, the agent will still be able to communicate and issue a new JWT to report back to Azure DevOps.There’s uncertainty on exactly how this works, and is probably maintained by Microsoft. I will however, try and do some more digging into this. The protected settings part of the settings file is encrypted, the VMSS instance has a computer certificate installed to decrypt the value.During the extension installation the protected settings on disk will be wiped after read.This can be intercepted in various ways, if you are interested to read the settings file.Part of the protected settings contains a JWT for the agent to authenticate to the Azure DevOps instance to call home.As the Azure DevOps agent also needs to install tools that might not exist on the machine, we will have to set the APT proxy as well.There’s different ways of setting the apt proxy, however to keep it simple I have chosen to create the apt.conf file at /etc/apt with the content:Acquire::http::Proxy \"http://10.2.0.7:3128\";This can either be done by the CustomScript extension or as part of a golden image capturing.Once all of these configurations have been set in place, then you will successfully be able to use your proxy and reach Azure DevOps/APT.Meaning the agent can now download the files needed, call home to Azure DevOps and download the dependencies needed to install successfully.I hope this helps to cast some clarity on how the Azure DevOps agent extension works and how it can be used behind a proxy.Thank you for reading and if you enjoy the content I highly suggest that you checkout some of my great colleagues that’s been part of figuring this out!Björn SundlingSimon Wåhlin" }, { "title": "Azure DevOps Pipeline Output", "url": "/posts/AzureDevOpsColoredOutput/", "categories": "Azure DevOps, Colored Output", "tags": "powershell, azure devops, output, ansi, color, pipeline", "date": "2022-09-03 08:55:00 +0200", "snippet": "Ever thought about visualising the output of your pipeline with color in Azure DevOps?I wanted to create my custom What-if deployment - PowerShell as there is an issue with the Azure Firewall rules...", "content": "Ever thought about visualising the output of your pipeline with color in Azure DevOps?I wanted to create my custom What-if deployment - PowerShell as there is an issue with the Azure Firewall rules being scrambled and giving false-positives for changes at each run, rendering the output almost unreadable.There’s built-in commands in Azure DevOps that can be used to produce colored output, sections and groups.Read more here: Azure DevOps Formatting CommandsThese formatting commands are great! The output renders such as:This acheives what we would like to see, however there’s only one little annoying thing, that is that there’s no informational (or success/verbose) command that we can use.The green output is called “Section”, which does not start with the ##[section] which makes it harder to read in the log output of Azure DevOps in my opinion, or if we could remove the ##[error] / ##[warning] of the orange/red outputs. I would simply like to have my error text in the color of my choosing and without any prefixes.The first thought that came to mind is that perhaps the write-host command with ForegroundColor might do the trick.However the rendered output from that command in Azure DevOps ends up like this:Which is not what we were looking for.I decided to test if the ANSI color codes would be rendered as documented here: ANSI Colors.To test this out, we create a yaml pipeline, as an example, the one below.steps:- pwsh: | $FormattedString = '{0}{1}{2}' -f \"$([char]27)[32`m\", 'Testing', \"$([char]27)[0m\" Write-output $FormattedStringAnd checking the rendered output in Azure DevOps you can see the following:We can see that it says ‘Testing’ in green!Checking the raw log we can also see this:This means that the Azure DevOps portal do render ANSI colors, which is great!After reviewing the wikipedia page regarding ANSI colors, we can also see that if we increase each color code by 10, it should also highlight the text with the color as a background color.Seems we’re onto something!We’re able to output colored text as we want, without any prefixes making it easier for a human to read.In this case I’ve also grouped the output in two sections, “ForegroundColored” and “BackgroundColored”.This is a great feature if you want to gather output in groups, making it easier to navigate the output and just expand the groups you are interested in.As I stated in the start, I’ve used this to build my own custom what-if deployment to render the Azure DevOps pipeline output humanly readable.I’ll create a blogpost series on that but for now the code for the demo pipeline and PowerShell function can be found on my github: SebastianClaesson - AzureDevOpsPipelineDemoA big thanks for reading and feel free to drop any comments below!" }, { "title": "PowerShell Azure Function - Managed identity & triggers/bindings", "url": "/posts/AzureFunctions/", "categories": "Azure, FunctionApp", "tags": "powershell, azure, alert, queue, table, identity, managed identity, function", "date": "2022-08-11 16:55:00 +0200", "snippet": "I’ve been playing around a little with Azure Functions and Azure Alerting.The design was to be able to utilize the Azure Alert rule processing function in Azure and have it create a prettier mail t...", "content": "I’ve been playing around a little with Azure Functions and Azure Alerting.The design was to be able to utilize the Azure Alert rule processing function in Azure and have it create a prettier mail than what comes out of box.Basically the design boiled down to having a few Azure Functions running a bit of PowerShell and a storage account with some tables and queues.First of all we’ll need a Function app with a managed identity.For this project I’ve decided to use a System Assigned identity for my Function app.I’ll deploy a bicep with the following settings to my Function app:var location = resourceGroup().locationresource storageAccount 'Microsoft.Storage/storageAccounts@2021-09-01' = { name: 'stalertdemo' kind: 'StorageV2' location: location sku: { name: 'Standard_LRS' } properties: { allowBlobPublicAccess: false allowSharedKeyAccess: true accessTier: 'Hot' supportsHttpsTrafficOnly: true }}resource queueService 'Microsoft.Storage/storageAccounts/queueServices@2021-09-01' = { name: 'default' parent: storageAccount}resource sendgrid 'Microsoft.Storage/storageAccounts/queueServices/queues@2021-09-01' = { name: 'sendgrid' parent: queueService}resource tableService 'Microsoft.Storage/storageAccounts/tableServices@2021-09-01' = { name: 'default' parent: storageAccount}resource keyvault 'Microsoft.KeyVault/vaults@2021-10-01' = { name: 'alertdemo-kv' location: location properties: { enabledForDeployment: false enabledForDiskEncryption: false enabledForTemplateDeployment: false enableRbacAuthorization: true enableSoftDelete: false tenantId: subscription().tenantId sku: { name: 'standard' family: 'A' } }}resource appServicePlan 'Microsoft.Web/serverfarms@2020-12-01' = { name: 'alertdemo-plan' location: location sku: { tier: 'Dynamic' name: 'Y1' }}resource functionApp 'Microsoft.Web/sites@2022-03-01' = { name: 'alertdemo-app' location: location kind: 'functionapp' identity: { type: 'SystemAssigned' } properties: { serverFarmId: appServicePlan.id httpsOnly: true clientAffinityEnabled: false siteConfig: { cors: { allowedOrigins: [ 'https://azure.portal.com' ] } powerShellVersion: '7.2' netFrameworkVersion: 'v6.0' ftpsState: 'Disabled' } }}resource functionAppSettings 'Microsoft.Web/sites/config@2020-06-01' = { parent: functionApp name: 'appsettings' properties: { AzureWebJobsDisableHomepage: 'true' AzureWebJobsSecretStorageKeyVaultUri: keyvault.properties.vaultUri AzureWebJobsSecretStorageType: 'keyvault' AzureWebJobsStorage__accountName: storageAccount.name FUNCTIONS_APP_EDIT_MODE: 'readonly' StorageQueueConnection__credential: 'managedidentity' StorageQueueConnection__queueServiceUri: storageAccount.properties.primaryEndpoints.queue WEBSITE_RUN_FROM_PACKAGE: '1' FUNCTIONS_WORKER_RUNTIME: 'powershell' FUNCTIONS_EXTENSION_VERSION: '~4' }}The bicep file will now create/configure: Create a Azure Storage Account with queue and table services enabled. Create a queue called sendgrid Store secrets in keyvault (such as the master/function keys) using the managed identity. Create a StorageQueueConnection object that will use my Storage Accounts queue endpoint and connect to that using the managed identity. The bicep file is not complete and is missing resources such as role assignments etc. This just a demonstration of how to set the application settings.Once the infrastructure is in place, you can now create the structure for your function app.If you’re familiar with zip deployments/function bindings, this will be easy.However, if you are not familiar with it, I suggest reading about it at Zip-deploy &amp; Function Triggers and Bindings.Brief explaination of the triggers/bindings.You can specify both in- and output bindings.There’s bindings that you can use from the open source community or included in the function runtime.Read more about the runtime extension bundle.In my scenario, I’ll create a input binding that uses the Azure Storage Queue trigger.For my function it will have a function.json in it’s folder, containing the following configuration:{ \"bindings\": [ { \"name\": \"QueueItem\", \"type\": \"queueTrigger\", \"direction\": \"in\", \"queueName\": \"sendgrid\", \"connection\": \"StorageQueueConnection\" } ]}As you can see there’s a queueTrigger binding called “QueueItem”.It also has a queueName which is the name of the queue, and a connection.The binding will use the StorageQueueConnection “object” specified in my function app settings to retrieve the connection, as we specified in the bicep template.The “object” can contain several settings, which you can read more about here Common properties for identity based connections &amp; Connecting to host storage with an identity (Preview).As you can see, I have no App Setting called “StorageQueueConnection”, however I do have the following app settings: StorageQueueConnection__credential: 'managedidentity' StorageQueueConnection__queueServiceUri: storageAccount.properties.primaryEndpoints.queueTogether these app settings form the connection object.When the runtime is running a sync cycle, it will try and parse my settings that is prefixed with “StorageQueueConnection” and followed by two underscores.In my case, it will try to connect to my Azure Storage Account Queue endpoint using the Function app managed identity. In order for this to work, you’ll have to make sure that the managed identity has at least the Storage Queue Data Contributor role.The configuration for the Azure Storage Account Queue settings can be set in the host.json file for the Function app.For our Function app, we’ll use the following configuration:{ \"version\": \"2.0\", \"extensionBundle\": { \"id\": \"Microsoft.Azure.Functions.ExtensionBundle\", \"version\": \"[3.11.0, 4.0.0)\" }, \"extensions\": { \"queues\": { \"maxPollingInterval\": \"00:00:02\", \"visibilityTimeout\": \"00:00:30\", \"batchSize\": 5, \"maxDequeueCount\": 5, \"newBatchThreshold\": 8, \"messageEncoding\": \"base64\" } }}This will configure our app to check the queue every 2 second.More about this can be found here queue extension settings.After adding some PowerShell magic to our function app it will now parse the data passed down by the queue to the function.using namespace System.Net# Input bindings are passed in via param block.param($QueueItem, $TriggerMetadata)Write-Information \"Queue item insertion time: $($TriggerMetadata.InsertionTime)\"Write-Information $QueueItem The parameter must match the name set in the function.json file, in our case ‘QueueItem’This PowerShell script will only output the insertion time and the data of the message found in the queue.We will make it more interesting in our case, and add a new output binding for SendGrid.To enable the output binding for SendGrid we will have to go back to our function.json file and add the following rows:{ \"bindings\": [ { \"name\": \"QueueItem\", \"type\": \"queueTrigger\", \"direction\": \"in\", \"queueName\": \"sendgrid\", \"connection\": \"StorageQueueConnection\" }, { \"type\": \"sendGrid\", \"direction\": \"out\", \"name\": \"message\", \"apikey\": \"&lt;input api key&gt;\" } ]}Now as you can see, we have added a new output binding of the type sendGrid in the out direction.We’ll give it the name message.To understand how the sendGrid output binding works, we can read the help docs at SendGrid configuration. To get your API key and getting started with SendGrid, you can read more here: SendGrid - Getting started. I strongly suggest that you keep this secret inside of an Azure Keyvault and have a reference to it inside your function app setting.But to keep it short, we’ll add the following code to our PowerShell script.$mail = @{ \"personalizations\" = @( @{ \"to\" = [System.Collections.ArrayList]@(@{\"email\" = \"user@contoso.com\"}) } ) \"from\" = @{ \"email\" = \"user@contoso.com\" } \"subject\" = \"New message!\" \"content\" = @( @{ \"type\" = \"text\" \"value\" = \"A new message was put on the queue!\" } )}# Send the email using the output binding for SendGrid.Push-OutputBinding -Name message -Value (ConvertTo-Json -InputObject $mail -Depth 100) Now the output binding will try to send an api call to SendGrid with our message.The simple message will look like this:To make it more advanced you can also send a html file, in my case I’ll use a html template file with some placeholders which we’ll search and replace during runtime and using the information passed down by the message.Simply change the change the content-type to “text/html” when posting to the output binding, if you like pretty mails :)Thank you for reading and hopefully my notes will be found useful!//Sebastian" } ]
